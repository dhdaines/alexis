{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import fasttext\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence, PackedSequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from poutyne import set_seeds, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingVectorizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Embedding vectorizer\n",
    "        \"\"\"\n",
    "        self.embedding_model = fasttext.load_model(\"../alexi/extraction/rnn99.fasttext\")\n",
    "\n",
    "    def __call__(self, address):\n",
    "        \"\"\"\n",
    "        Convert address to embedding vectors\n",
    "        :param address: The address to convert\n",
    "        :return: The embeddings vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for word in address.split():\n",
    "            embeddings.append(self.embedding_model[word])\n",
    "        return embeddings\n",
    "embedding_model = EmbeddingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alexi.pagernn import PageDataset, simplify_tags\n",
    "from pathlib import Path\n",
    "ds = PageDataset(Path(\"../data/train/\").glob(\"*.csv\"), label_transform=simplify_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 128 13\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "lr = 0.001\n",
    "dimension = ds.pages[0][0].shape[1]\n",
    "num_layer = 1\n",
    "bidirectional = True\n",
    "hidden_size = 64\n",
    "\n",
    "lstm_network = nn.LSTM(\n",
    "    input_size=dimension,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layer,\n",
    "    bidirectional=bidirectional,\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "input_dim = hidden_size * 2 if bidirectional else 1\n",
    "tag_dimension = max(ds.vocab.values()) + 1\n",
    "fully_connected_network = nn.Linear(input_dim, tag_dimension)\n",
    "print(dimension, input_dim, tag_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = torch.utils.data.random_split(ds, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    The collate_fn that can add padding to the sequences so all can have\n",
    "    the same length as the longest one.\n",
    "\n",
    "    Args:\n",
    "        batch (List[List, List]): The batch data, where the first element\n",
    "        of the tuple are the word idx and the second element are the target\n",
    "        label.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (x, y). The element x is a tensor of packed sequence .\n",
    "        The element y is a tensor of padded tag indices. The word vectors are\n",
    "        padded with vectors of 0s and the tag indices are padded with -100s.\n",
    "        Padding with -100 is done because of the cross-entropy loss and the\n",
    "        accuracy metric ignores the targets with values -100.\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets us two lists of tensors and a list of integer.\n",
    "    # Each tensor in the first list is a sequence of word vectors.\n",
    "    # Each tensor in the second list is a sequence of tag indices.\n",
    "    # The list of integer consist of the lengths of the sequences in order.\n",
    "    sequences_vectors, sequences_labels, lengths = zip(\n",
    "        *[\n",
    "            (torch.FloatTensor(np.stack(seq_vectors)), torch.LongTensor(labels), len(seq_vectors))\n",
    "            for (seq_vectors, labels) in sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    padded_sequences_vectors = pad_sequence(sequences_vectors, batch_first=True, padding_value=0)\n",
    "    pack_padded_sequences_vectors = pack_padded_sequence(\n",
    "        padded_sequences_vectors, lengths.cpu(), batch_first=True\n",
    "    )  # We pack the padded sequence to improve the computational speed during training\n",
    "\n",
    "    padded_sequences_labels = pad_sequence(sequences_labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return pack_padded_sequences_vectors, padded_sequences_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_collate_fn,\n",
    ")\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   8,    2,    2,  ...,    2,    2,    2],\n",
       "        [   5,    2,    2,  ..., -100, -100, -100],\n",
       "        [   5,    2,    2,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [   4,    2,    2,  ..., -100, -100, -100],\n",
       "        [   4,    2,    2,  ..., -100, -100, -100],\n",
       "        [   4,    2,    2,  ..., -100, -100, -100]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors, labels = next(iter(train_loader))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FullNetWork(nn.Module):\n",
    "    def __init__(self, lstm_network, fully_connected_network):\n",
    "        super().__init__()\n",
    "        self.hidden_state = None\n",
    "\n",
    "        self.lstm_network = lstm_network\n",
    "        self.fully_connected_network = fully_connected_network\n",
    "\n",
    "    def forward(self, pack_padded_sequences_vectors: PackedSequence):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "        lstm_out, self.hidden_state = self.lstm_network(pack_padded_sequences_vectors)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        tag_space = self.fully_connected_network(lstm_out)\n",
    "        return tag_space.transpose(-1, 1)  # We need to transpose since it's a sequence\n",
    "\n",
    "\n",
    "full_network = FullNetWork(lstm_network, fully_connected_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/100 Train steps: 8 Val steps: 1 1.44s loss: 2.262218 acc: 47.396558 val_loss: 1.842793 val_acc: 86.662842\n",
      "Epoch:   2/100 Train steps: 8 Val steps: 1 1.41s loss: 1.599728 acc: 85.504379 val_loss: 1.242491 val_acc: 86.873322\n",
      "Epoch:   3/100 Train steps: 8 Val steps: 1 1.27s loss: 1.077600 acc: 85.212161 val_loss: 0.808188 val_acc: 86.892464\n",
      "Epoch:   4/100 Train steps: 8 Val steps: 1 1.28s loss: 0.722276 acc: 85.614225 val_loss: 0.554010 val_acc: 86.969002\n",
      "Epoch:   5/100 Train steps: 8 Val steps: 1 1.27s loss: 0.548160 acc: 85.026287 val_loss: 0.424166 val_acc: 87.198624\n",
      "Epoch:   6/100 Train steps: 8 Val steps: 1 1.38s loss: 0.460802 acc: 84.737669 val_loss: 0.351254 val_acc: 87.543053\n",
      "Epoch:   7/100 Train steps: 8 Val steps: 1 1.27s loss: 0.404213 acc: 85.495909 val_loss: 0.304790 val_acc: 88.136246\n",
      "Epoch:   8/100 Train steps: 8 Val steps: 1 1.27s loss: 0.363777 acc: 86.799857 val_loss: 0.259612 val_acc: 90.604668\n",
      "Epoch:   9/100 Train steps: 8 Val steps: 1 1.26s loss: 0.337801 acc: 88.651087 val_loss: 0.231080 val_acc: 92.958282\n",
      "Epoch:  10/100 Train steps: 8 Val steps: 1 1.42s loss: 0.316862 acc: 89.160629 val_loss: 0.203409 val_acc: 94.565636\n",
      "Epoch:  11/100 Train steps: 8 Val steps: 1 1.33s loss: 0.290803 acc: 90.528246 val_loss: 0.184009 val_acc: 96.153847\n",
      "Epoch:  12/100 Train steps: 8 Val steps: 1 1.35s loss: 0.270274 acc: 91.313692 val_loss: 0.159621 val_acc: 96.880981\n",
      "Epoch:  13/100 Train steps: 8 Val steps: 1 1.29s loss: 0.263796 acc: 91.961691 val_loss: 0.154871 val_acc: 97.914276\n",
      "Epoch:  14/100 Train steps: 8 Val steps: 1 1.41s loss: 0.249125 acc: 92.094118 val_loss: 0.150365 val_acc: 97.703789\n",
      "Epoch:  15/100 Train steps: 8 Val steps: 1 1.28s loss: 0.226264 acc: 92.451855 val_loss: 0.129147 val_acc: 97.914276\n",
      "Epoch:  16/100 Train steps: 8 Val steps: 1 1.30s loss: 0.236855 acc: 92.239387 val_loss: 0.112411 val_acc: 98.296974\n",
      "Epoch:  17/100 Train steps: 8 Val steps: 1 1.46s loss: 0.214898 acc: 92.982876 val_loss: 0.121213 val_acc: 98.622276\n",
      "Epoch:  18/100 Train steps: 8 Val steps: 1 1.29s loss: 0.198161 acc: 93.724651 val_loss: 0.097539 val_acc: 98.679680\n",
      "Epoch:  19/100 Train steps: 8 Val steps: 1 1.27s loss: 0.193114 acc: 93.369868 val_loss: 0.093536 val_acc: 98.679680\n",
      "Epoch:  20/100 Train steps: 8 Val steps: 1 1.26s loss: 0.188693 acc: 93.313593 val_loss: 0.088643 val_acc: 98.832764\n",
      "Epoch:  21/100 Train steps: 8 Val steps: 1 1.41s loss: 0.169757 acc: 94.234272 val_loss: 0.074666 val_acc: 98.871025\n",
      "Epoch:  22/100 Train steps: 8 Val steps: 1 1.27s loss: 0.178240 acc: 94.307919 val_loss: 0.069978 val_acc: 98.890167\n",
      "Epoch:  23/100 Train steps: 8 Val steps: 1 1.30s loss: 0.172772 acc: 94.166214 val_loss: 0.100517 val_acc: 98.641411\n",
      "Epoch:  24/100 Train steps: 8 Val steps: 1 1.30s loss: 0.160281 acc: 94.483464 val_loss: 0.067771 val_acc: 98.890167\n",
      "Epoch:  25/100 Train steps: 8 Val steps: 1 1.45s loss: 0.166016 acc: 94.759524 val_loss: 0.100871 val_acc: 98.928436\n",
      "Epoch:  26/100 Train steps: 8 Val steps: 1 1.27s loss: 0.166866 acc: 94.819435 val_loss: 0.056391 val_acc: 98.928436\n",
      "Epoch:  27/100 Train steps: 8 Val steps: 1 1.35s loss: 0.153186 acc: 94.731812 val_loss: 0.062350 val_acc: 98.985840\n",
      "Epoch:  28/100 Train steps: 8 Val steps: 1 1.27s loss: 0.151461 acc: 94.649147 val_loss: 0.079944 val_acc: 99.024109\n",
      "Epoch:  29/100 Train steps: 8 Val steps: 1 1.41s loss: 0.140137 acc: 95.755849 val_loss: 0.050448 val_acc: 99.024109\n",
      "Epoch:  30/100 Train steps: 8 Val steps: 1 1.27s loss: 0.142155 acc: 94.907215 val_loss: 0.061833 val_acc: 98.947571\n",
      "Epoch:  31/100 Train steps: 8 Val steps: 1 1.29s loss: 0.127700 acc: 95.486800 val_loss: 0.053574 val_acc: 99.177193\n",
      "Epoch:  32/100 Train steps: 8 Val steps: 1 1.28s loss: 0.120728 acc: 96.025259 val_loss: 0.045531 val_acc: 99.119789\n",
      "Epoch:  33/100 Train steps: 8 Val steps: 1 1.41s loss: 0.110692 acc: 95.969058 val_loss: 0.059788 val_acc: 99.043243\n",
      "Epoch:  34/100 Train steps: 8 Val steps: 1 1.27s loss: 0.119202 acc: 95.149186 val_loss: 0.040432 val_acc: 99.253731\n",
      "Epoch:  35/100 Train steps: 8 Val steps: 1 1.29s loss: 0.115408 acc: 96.103848 val_loss: 0.060574 val_acc: 99.024109\n",
      "Epoch:  36/100 Train steps: 8 Val steps: 1 1.29s loss: 0.106999 acc: 96.052680 val_loss: 0.039118 val_acc: 99.138924\n",
      "Epoch:  37/100 Train steps: 8 Val steps: 1 1.42s loss: 0.116565 acc: 95.657632 val_loss: 0.081410 val_acc: 98.928436\n",
      "Epoch:  38/100 Train steps: 8 Val steps: 1 1.28s loss: 0.132512 acc: 95.804113 val_loss: 0.042815 val_acc: 99.234596\n",
      "Epoch:  39/100 Train steps: 8 Val steps: 1 1.27s loss: 0.119258 acc: 95.856833 val_loss: 0.049812 val_acc: 99.043243\n",
      "Epoch:  40/100 Train steps: 8 Val steps: 1 1.28s loss: 0.110057 acc: 96.100188 val_loss: 0.058700 val_acc: 98.985840\n",
      "Epoch:  41/100 Train steps: 8 Val steps: 1 1.42s loss: 0.103327 acc: 96.617339 val_loss: 0.040589 val_acc: 99.253731\n",
      "Epoch:  42/100 Train steps: 8 Val steps: 1 1.27s loss: 0.099093 acc: 96.460050 val_loss: 0.037878 val_acc: 99.234596\n",
      "Epoch:  43/100 Train steps: 8 Val steps: 1 1.29s loss: 0.096350 acc: 96.483768 val_loss: 0.039993 val_acc: 99.234596\n",
      "Epoch:  44/100 Train steps: 8 Val steps: 1 1.29s loss: 0.093412 acc: 96.650296 val_loss: 0.034888 val_acc: 99.196327\n",
      "Epoch:  45/100 Train steps: 8 Val steps: 1 1.44s loss: 0.085378 acc: 97.030399 val_loss: 0.037142 val_acc: 99.177193\n",
      "Epoch:  46/100 Train steps: 8 Val steps: 1 1.38s loss: 0.083488 acc: 97.035832 val_loss: 0.032236 val_acc: 99.272865\n",
      "Epoch:  47/100 Train steps: 8 Val steps: 1 1.33s loss: 0.080224 acc: 97.286554 val_loss: 0.032008 val_acc: 99.158058\n",
      "Epoch:  48/100 Train steps: 8 Val steps: 1 1.27s loss: 0.084121 acc: 96.967206 val_loss: 0.032640 val_acc: 99.234596\n",
      "Epoch:  49/100 Train steps: 8 Val steps: 1 1.47s loss: 0.080574 acc: 96.856129 val_loss: 0.035465 val_acc: 99.158058\n",
      "Epoch:  50/100 Train steps: 8 Val steps: 1 1.37s loss: 0.081986 acc: 96.883822 val_loss: 0.036567 val_acc: 99.081512\n",
      "Epoch:  51/100 Train steps: 8 Val steps: 1 1.28s loss: 0.078318 acc: 97.217060 val_loss: 0.030065 val_acc: 99.311134\n",
      "Epoch:  52/100 Train steps: 8 Val steps: 1 1.27s loss: 0.078188 acc: 97.366472 val_loss: 0.029469 val_acc: 99.311134\n",
      "Epoch:  53/100 Train steps: 8 Val steps: 1 1.41s loss: 0.079578 acc: 96.945764 val_loss: 0.034193 val_acc: 99.119789\n",
      "Epoch:  54/100 Train steps: 8 Val steps: 1 1.28s loss: 0.076441 acc: 97.227487 val_loss: 0.027828 val_acc: 99.292007\n",
      "Epoch:  55/100 Train steps: 8 Val steps: 1 1.28s loss: 0.079469 acc: 96.955579 val_loss: 0.068308 val_acc: 98.450058\n",
      "Epoch:  56/100 Train steps: 8 Val steps: 1 1.28s loss: 0.080923 acc: 97.195092 val_loss: 0.025060 val_acc: 99.425949\n",
      "Epoch:  57/100 Train steps: 8 Val steps: 1 1.45s loss: 0.076025 acc: 97.169787 val_loss: 0.023270 val_acc: 99.464218\n",
      "Epoch:  58/100 Train steps: 8 Val steps: 1 1.31s loss: 0.081029 acc: 97.079623 val_loss: 0.044368 val_acc: 98.909302\n",
      "Epoch:  59/100 Train steps: 8 Val steps: 1 1.28s loss: 0.078228 acc: 97.119430 val_loss: 0.025189 val_acc: 99.387672\n",
      "Epoch:  60/100 Train steps: 8 Val steps: 1 1.30s loss: 0.065908 acc: 97.607352 val_loss: 0.025731 val_acc: 99.368546\n",
      "Epoch:  61/100 Train steps: 8 Val steps: 1 1.40s loss: 0.060698 acc: 97.852676 val_loss: 0.031896 val_acc: 99.043243\n",
      "Epoch:  62/100 Train steps: 8 Val steps: 1 1.27s loss: 0.062492 acc: 97.724534 val_loss: 0.028187 val_acc: 99.234596\n",
      "Epoch:  63/100 Train steps: 8 Val steps: 1 1.30s loss: 0.065262 acc: 97.707035 val_loss: 0.021160 val_acc: 99.483353\n",
      "Epoch:  64/100 Train steps: 8 Val steps: 1 1.40s loss: 0.059780 acc: 98.180691 val_loss: 0.026366 val_acc: 99.253731\n",
      "Epoch:  65/100 Train steps: 8 Val steps: 1 1.31s loss: 0.061374 acc: 97.685577 val_loss: 0.033629 val_acc: 99.100647\n",
      "Epoch:  66/100 Train steps: 8 Val steps: 1 1.30s loss: 0.058587 acc: 98.150968 val_loss: 0.019774 val_acc: 99.521622\n",
      "Epoch:  67/100 Train steps: 8 Val steps: 1 1.29s loss: 0.055420 acc: 98.018695 val_loss: 0.030178 val_acc: 99.100647\n",
      "Epoch:  68/100 Train steps: 8 Val steps: 1 1.40s loss: 0.058928 acc: 98.041970 val_loss: 0.023791 val_acc: 99.311134\n",
      "Epoch:  69/100 Train steps: 8 Val steps: 1 1.33s loss: 0.053379 acc: 98.265633 val_loss: 0.021510 val_acc: 99.349411\n",
      "Epoch:  70/100 Train steps: 8 Val steps: 1 1.26s loss: 0.063928 acc: 97.610377 val_loss: 0.020940 val_acc: 99.483353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  71/100 Train steps: 8 Val steps: 1 1.26s loss: 0.055486 acc: 98.017498 val_loss: 0.022193 val_acc: 99.559891\n",
      "Epoch:  72/100 Train steps: 8 Val steps: 1 1.42s loss: 0.048630 acc: 98.588574 val_loss: 0.020154 val_acc: 99.483353\n",
      "Epoch:  73/100 Train steps: 8 Val steps: 1 1.28s loss: 0.048580 acc: 98.289996 val_loss: 0.024494 val_acc: 99.425949\n",
      "Epoch:  74/100 Train steps: 8 Val steps: 1 1.27s loss: 0.047179 acc: 98.586150 val_loss: 0.017982 val_acc: 99.521622\n",
      "Epoch:  75/100 Train steps: 8 Val steps: 1 1.28s loss: 0.050885 acc: 98.269296 val_loss: 0.019824 val_acc: 99.502487\n",
      "Epoch:  76/100 Train steps: 8 Val steps: 1 1.41s loss: 0.054173 acc: 97.989514 val_loss: 0.031833 val_acc: 99.330269\n",
      "Epoch:  77/100 Train steps: 8 Val steps: 1 1.28s loss: 0.050529 acc: 98.455178 val_loss: 0.017366 val_acc: 99.502487\n",
      "Epoch:  78/100 Train steps: 8 Val steps: 1 1.27s loss: 0.056244 acc: 97.893055 val_loss: 0.023064 val_acc: 99.483353\n",
      "Epoch:  79/100 Train steps: 8 Val steps: 1 1.28s loss: 0.045725 acc: 98.410302 val_loss: 0.019144 val_acc: 99.579033\n",
      "Epoch:  80/100 Train steps: 8 Val steps: 1 1.44s loss: 0.042827 acc: 98.704369 val_loss: 0.020923 val_acc: 99.425949\n",
      "Epoch:  81/100 Train steps: 8 Val steps: 1 1.27s loss: 0.042458 acc: 98.500326 val_loss: 0.021488 val_acc: 99.521622\n",
      "Epoch:  82/100 Train steps: 8 Val steps: 1 1.28s loss: 0.045835 acc: 98.492829 val_loss: 0.018167 val_acc: 99.521622\n",
      "Epoch:  83/100 Train steps: 8 Val steps: 1 1.28s loss: 0.041808 acc: 98.583775 val_loss: 0.024453 val_acc: 99.368546\n",
      "Epoch:  84/100 Train steps: 8 Val steps: 1 1.39s loss: 0.047536 acc: 98.281175 val_loss: 0.020863 val_acc: 99.483353\n",
      "Epoch:  85/100 Train steps: 8 Val steps: 1 1.35s loss: 0.059511 acc: 97.958753 val_loss: 0.018279 val_acc: 99.598160\n",
      "Epoch:  86/100 Train steps: 8 Val steps: 1 1.27s loss: 0.074422 acc: 97.288247 val_loss: 0.025682 val_acc: 99.425949\n",
      "Epoch:  87/100 Train steps: 8 Val steps: 1 1.28s loss: 0.052570 acc: 98.122218 val_loss: 0.028936 val_acc: 99.406815\n",
      "Epoch:  88/100 Train steps: 8 Val steps: 1 1.42s loss: 0.051746 acc: 98.243869 val_loss: 0.017639 val_acc: 99.617294\n",
      "Epoch:  89/100 Train steps: 8 Val steps: 1 1.33s loss: 0.044513 acc: 98.477132 val_loss: 0.022033 val_acc: 99.483353\n",
      "Epoch:  90/100 Train steps: 8 Val steps: 1 1.30s loss: 0.042711 acc: 98.600807 val_loss: 0.017477 val_acc: 99.579033\n",
      "Epoch:  91/100 Train steps: 8 Val steps: 1 1.27s loss: 0.037250 acc: 98.917400 val_loss: 0.019217 val_acc: 99.540756\n",
      "Epoch:  92/100 Train steps: 8 Val steps: 1 1.40s loss: 0.035375 acc: 98.948673 val_loss: 0.017027 val_acc: 99.598160\n",
      "Epoch:  93/100 Train steps: 8 Val steps: 1 1.28s loss: 0.034368 acc: 99.013763 val_loss: 0.018712 val_acc: 99.559891\n",
      "Epoch:  94/100 Train steps: 8 Val steps: 1 1.27s loss: 0.035890 acc: 98.949702 val_loss: 0.016391 val_acc: 99.579033\n",
      "Epoch:  95/100 Train steps: 8 Val steps: 1 1.26s loss: 0.037256 acc: 98.906803 val_loss: 0.020450 val_acc: 99.406815\n",
      "Epoch:  96/100 Train steps: 8 Val steps: 1 1.41s loss: 0.035001 acc: 98.933851 val_loss: 0.018657 val_acc: 99.579033\n",
      "Epoch:  97/100 Train steps: 8 Val steps: 1 1.29s loss: 0.035475 acc: 98.905924 val_loss: 0.016703 val_acc: 99.598160\n",
      "Epoch:  98/100 Train steps: 8 Val steps: 1 1.28s loss: 0.037266 acc: 98.860792 val_loss: 0.018330 val_acc: 99.598160\n",
      "Epoch:  99/100 Train steps: 8 Val steps: 1 1.27s loss: 0.035925 acc: 98.804696 val_loss: 0.018411 val_acc: 99.655571\n",
      "Epoch: 100/100 Train steps: 8 Val steps: 1 1.42s loss: 0.032699 acc: 99.117028 val_loss: 0.014299 val_acc: 99.751244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'time': 1.4401972999912687,\n",
       "  'loss': 2.2622180871727053,\n",
       "  'acc': 47.39655817835784,\n",
       "  'val_loss': 1.8427928686141968,\n",
       "  'val_acc': 86.662841796875},\n",
       " {'epoch': 2,\n",
       "  'time': 1.4064530259929597,\n",
       "  'loss': 1.5997280591775562,\n",
       "  'acc': 85.50437889414385,\n",
       "  'val_loss': 1.2424908876419067,\n",
       "  'val_acc': 86.87332153320312},\n",
       " {'epoch': 3,\n",
       "  'time': 1.2650642509979662,\n",
       "  'loss': 1.077599920517157,\n",
       "  'acc': 85.21216072523889,\n",
       "  'val_loss': 0.8081880807876587,\n",
       "  'val_acc': 86.89246368408203},\n",
       " {'epoch': 4,\n",
       "  'time': 1.2774836199969286,\n",
       "  'loss': 0.7222762575819472,\n",
       "  'acc': 85.61422477280799,\n",
       "  'val_loss': 0.5540096163749695,\n",
       "  'val_acc': 86.96900177001953},\n",
       " {'epoch': 5,\n",
       "  'time': 1.2725920679949922,\n",
       "  'loss': 0.5481603633273732,\n",
       "  'acc': 85.02628673206677,\n",
       "  'val_loss': 0.4241655468940735,\n",
       "  'val_acc': 87.19862365722656},\n",
       " {'epoch': 6,\n",
       "  'time': 1.375001530002919,\n",
       "  'loss': 0.46080178490354995,\n",
       "  'acc': 84.73766888074638,\n",
       "  'val_loss': 0.35125401616096497,\n",
       "  'val_acc': 87.54305267333984},\n",
       " {'epoch': 7,\n",
       "  'time': 1.2701964910083916,\n",
       "  'loss': 0.40421327086519604,\n",
       "  'acc': 85.49590932042146,\n",
       "  'val_loss': 0.30478963255882263,\n",
       "  'val_acc': 88.13624572753906},\n",
       " {'epoch': 8,\n",
       "  'time': 1.2695303490036167,\n",
       "  'loss': 0.3637768095436175,\n",
       "  'acc': 86.79985676915193,\n",
       "  'val_loss': 0.25961175560951233,\n",
       "  'val_acc': 90.60466766357422},\n",
       " {'epoch': 9,\n",
       "  'time': 1.2582446199958213,\n",
       "  'loss': 0.3378006615175689,\n",
       "  'acc': 88.65108729591054,\n",
       "  'val_loss': 0.2310796082019806,\n",
       "  'val_acc': 92.95828247070312},\n",
       " {'epoch': 10,\n",
       "  'time': 1.4218571680103196,\n",
       "  'loss': 0.31686194376512006,\n",
       "  'acc': 89.16062920940809,\n",
       "  'val_loss': 0.20340853929519653,\n",
       "  'val_acc': 94.56563568115234},\n",
       " {'epoch': 11,\n",
       "  'time': 1.3327654219901888,\n",
       "  'loss': 0.29080305217711394,\n",
       "  'acc': 90.52824559487587,\n",
       "  'val_loss': 0.18400901556015015,\n",
       "  'val_acc': 96.15384674072266},\n",
       " {'epoch': 12,\n",
       "  'time': 1.3469120529916836,\n",
       "  'loss': 0.27027374789241915,\n",
       "  'acc': 91.31369157270952,\n",
       "  'val_loss': 0.1596205234527588,\n",
       "  'val_acc': 96.8809814453125},\n",
       " {'epoch': 13,\n",
       "  'time': 1.2911600609950256,\n",
       "  'loss': 0.2637960319676675,\n",
       "  'acc': 91.96169054015608,\n",
       "  'val_loss': 0.15487080812454224,\n",
       "  'val_acc': 97.91427612304688},\n",
       " {'epoch': 14,\n",
       "  'time': 1.4054723669978557,\n",
       "  'loss': 0.2491253978949933,\n",
       "  'acc': 92.09411822862862,\n",
       "  'val_loss': 0.15036532282829285,\n",
       "  'val_acc': 97.70378875732422},\n",
       " {'epoch': 15,\n",
       "  'time': 1.2841265620081685,\n",
       "  'loss': 0.22626360216416602,\n",
       "  'acc': 92.45185454817843,\n",
       "  'val_loss': 0.12914706766605377,\n",
       "  'val_acc': 97.91427612304688},\n",
       " {'epoch': 16,\n",
       "  'time': 1.3012397269922076,\n",
       "  'loss': 0.23685541293345208,\n",
       "  'acc': 92.23938662946716,\n",
       "  'val_loss': 0.1124105378985405,\n",
       "  'val_acc': 98.2969741821289},\n",
       " {'epoch': 17,\n",
       "  'time': 1.4593478590104496,\n",
       "  'loss': 0.2148980710132063,\n",
       "  'acc': 92.98287585550104,\n",
       "  'val_loss': 0.12121345847845078,\n",
       "  'val_acc': 98.62227630615234},\n",
       " {'epoch': 18,\n",
       "  'time': 1.2906145930028288,\n",
       "  'loss': 0.1981605701456385,\n",
       "  'acc': 93.72465092682641,\n",
       "  'val_loss': 0.09753900021314621,\n",
       "  'val_acc': 98.67967987060547},\n",
       " {'epoch': 19,\n",
       "  'time': 1.2704498870007228,\n",
       "  'loss': 0.19311409517506922,\n",
       "  'acc': 93.36986819180575,\n",
       "  'val_loss': 0.0935356467962265,\n",
       "  'val_acc': 98.67967987060547},\n",
       " {'epoch': 20,\n",
       "  'time': 1.261177337000845,\n",
       "  'loss': 0.18869297982247407,\n",
       "  'acc': 93.313592642792,\n",
       "  'val_loss': 0.08864279836416245,\n",
       "  'val_acc': 98.832763671875},\n",
       " {'epoch': 21,\n",
       "  'time': 1.4095426880085142,\n",
       "  'loss': 0.16975709857526888,\n",
       "  'acc': 94.23427178248886,\n",
       "  'val_loss': 0.07466558367013931,\n",
       "  'val_acc': 98.87102508544922},\n",
       " {'epoch': 22,\n",
       "  'time': 1.2730431939999107,\n",
       "  'loss': 0.17823975613294554,\n",
       "  'acc': 94.30791939979743,\n",
       "  'val_loss': 0.06997814029455185,\n",
       "  'val_acc': 98.89016723632812},\n",
       " {'epoch': 23,\n",
       "  'time': 1.3027784160076408,\n",
       "  'loss': 0.1727718744149878,\n",
       "  'acc': 94.1662143675749,\n",
       "  'val_loss': 0.1005168929696083,\n",
       "  'val_acc': 98.64141082763672},\n",
       " {'epoch': 24,\n",
       "  'time': 1.2969257479999214,\n",
       "  'loss': 0.16028066336616012,\n",
       "  'acc': 94.48346401246125,\n",
       "  'val_loss': 0.06777088344097137,\n",
       "  'val_acc': 98.89016723632812},\n",
       " {'epoch': 25,\n",
       "  'time': 1.452986468008021,\n",
       "  'loss': 0.16601583657185892,\n",
       "  'acc': 94.75952425870028,\n",
       "  'val_loss': 0.10087108612060547,\n",
       "  'val_acc': 98.92843627929688},\n",
       " {'epoch': 26,\n",
       "  'time': 1.2680260710039875,\n",
       "  'loss': 0.16686610446488562,\n",
       "  'acc': 94.81943499352322,\n",
       "  'val_loss': 0.056390754878520966,\n",
       "  'val_acc': 98.92843627929688},\n",
       " {'epoch': 27,\n",
       "  'time': 1.3466703930025687,\n",
       "  'loss': 0.15318649083622232,\n",
       "  'acc': 94.73181202786027,\n",
       "  'val_loss': 0.062349945306777954,\n",
       "  'val_acc': 98.98583984375},\n",
       " {'epoch': 28,\n",
       "  'time': 1.2744320229976438,\n",
       "  'loss': 0.15146124584615722,\n",
       "  'acc': 94.64914697063855,\n",
       "  'val_loss': 0.07994397729635239,\n",
       "  'val_acc': 99.02410888671875},\n",
       " {'epoch': 29,\n",
       "  'time': 1.4087625560059678,\n",
       "  'loss': 0.14013691517439755,\n",
       "  'acc': 95.75584922349158,\n",
       "  'val_loss': 0.05044793337583542,\n",
       "  'val_acc': 99.02410888671875},\n",
       " {'epoch': 30,\n",
       "  'time': 1.2746840110048652,\n",
       "  'loss': 0.14215526206434265,\n",
       "  'acc': 94.90721533909317,\n",
       "  'val_loss': 0.06183256581425667,\n",
       "  'val_acc': 98.94757080078125},\n",
       " {'epoch': 31,\n",
       "  'time': 1.2910452450014418,\n",
       "  'loss': 0.1276995237331745,\n",
       "  'acc': 95.48680039082677,\n",
       "  'val_loss': 0.053573619574308395,\n",
       "  'val_acc': 99.17719268798828},\n",
       " {'epoch': 32,\n",
       "  'time': 1.277605928000412,\n",
       "  'loss': 0.12072806134204234,\n",
       "  'acc': 96.0252589706547,\n",
       "  'val_loss': 0.04553113505244255,\n",
       "  'val_acc': 99.11978912353516},\n",
       " {'epoch': 33,\n",
       "  'time': 1.4093659720092546,\n",
       "  'loss': 0.11069167196011741,\n",
       "  'acc': 95.96905801315938,\n",
       "  'val_loss': 0.059787567704916,\n",
       "  'val_acc': 99.04324340820312},\n",
       " {'epoch': 34,\n",
       "  'time': 1.2749059399939142,\n",
       "  'loss': 0.1192015894680969,\n",
       "  'acc': 95.14918574813969,\n",
       "  'val_loss': 0.0404323935508728,\n",
       "  'val_acc': 99.25373077392578},\n",
       " {'epoch': 35,\n",
       "  'time': 1.2867447879980318,\n",
       "  'loss': 0.11540827746233664,\n",
       "  'acc': 96.10384848098124,\n",
       "  'val_loss': 0.06057412922382355,\n",
       "  'val_acc': 99.02410888671875},\n",
       " {'epoch': 36,\n",
       "  'time': 1.2856318239937536,\n",
       "  'loss': 0.10699910775196454,\n",
       "  'acc': 96.05268040176266,\n",
       "  'val_loss': 0.039118457585573196,\n",
       "  'val_acc': 99.13892364501953},\n",
       " {'epoch': 37,\n",
       "  'time': 1.4158049760007998,\n",
       "  'loss': 0.1165653511512378,\n",
       "  'acc': 95.65763180315003,\n",
       "  'val_loss': 0.08140967786312103,\n",
       "  'val_acc': 98.92843627929688},\n",
       " {'epoch': 38,\n",
       "  'time': 1.282910906986217,\n",
       "  'loss': 0.13251248396132603,\n",
       "  'acc': 95.80411289940196,\n",
       "  'val_loss': 0.0428151935338974,\n",
       "  'val_acc': 99.2345962524414},\n",
       " {'epoch': 39,\n",
       "  'time': 1.2702625219972106,\n",
       "  'loss': 0.11925776376704539,\n",
       "  'acc': 95.85683264614137,\n",
       "  'val_loss': 0.04981166496872902,\n",
       "  'val_acc': 99.04324340820312},\n",
       " {'epoch': 40,\n",
       "  'time': 1.2784570639923913,\n",
       "  'loss': 0.11005678354216016,\n",
       "  'acc': 96.10018794792742,\n",
       "  'val_loss': 0.05870009586215019,\n",
       "  'val_acc': 98.98583984375},\n",
       " {'epoch': 41,\n",
       "  'time': 1.4240062729950296,\n",
       "  'loss': 0.10332693689125629,\n",
       "  'acc': 96.61733933125646,\n",
       "  'val_loss': 0.040589407086372375,\n",
       "  'val_acc': 99.25373077392578},\n",
       " {'epoch': 42,\n",
       "  'time': 1.2730259190138895,\n",
       "  'loss': 0.09909251049037807,\n",
       "  'acc': 96.46005009422618,\n",
       "  'val_loss': 0.03787769377231598,\n",
       "  'val_acc': 99.2345962524414},\n",
       " {'epoch': 43,\n",
       "  'time': 1.2858907290064963,\n",
       "  'loss': 0.09635034609924663,\n",
       "  'acc': 96.48376817939695,\n",
       "  'val_loss': 0.03999349847435951,\n",
       "  'val_acc': 99.2345962524414},\n",
       " {'epoch': 44,\n",
       "  'time': 1.2866514719935367,\n",
       "  'loss': 0.0934124040948458,\n",
       "  'acc': 96.65029617183464,\n",
       "  'val_loss': 0.034887608140707016,\n",
       "  'val_acc': 99.19632720947266},\n",
       " {'epoch': 45,\n",
       "  'time': 1.4359995230042841,\n",
       "  'loss': 0.0853777272272701,\n",
       "  'acc': 97.03039866045486,\n",
       "  'val_loss': 0.037142470479011536,\n",
       "  'val_acc': 99.17719268798828},\n",
       " {'epoch': 46,\n",
       "  'time': 1.3841864529968007,\n",
       "  'loss': 0.08348787305029956,\n",
       "  'acc': 97.03583205041807,\n",
       "  'val_loss': 0.032235823571681976,\n",
       "  'val_acc': 99.27286529541016},\n",
       " {'epoch': 47,\n",
       "  'time': 1.3340691280027386,\n",
       "  'loss': 0.08022429627820479,\n",
       "  'acc': 97.28655438383748,\n",
       "  'val_loss': 0.03200789541006088,\n",
       "  'val_acc': 99.1580581665039},\n",
       " {'epoch': 48,\n",
       "  'time': 1.2694698109989986,\n",
       "  'loss': 0.08412107517404005,\n",
       "  'acc': 96.9672060879794,\n",
       "  'val_loss': 0.032639939337968826,\n",
       "  'val_acc': 99.2345962524414},\n",
       " {'epoch': 49,\n",
       "  'time': 1.4712569990078919,\n",
       "  'loss': 0.08057428951844696,\n",
       "  'acc': 96.85612910247046,\n",
       "  'val_loss': 0.03546486422419548,\n",
       "  'val_acc': 99.1580581665039},\n",
       " {'epoch': 50,\n",
       "  'time': 1.3688979869912146,\n",
       "  'loss': 0.08198590379608564,\n",
       "  'acc': 96.88382159776924,\n",
       "  'val_loss': 0.03656722605228424,\n",
       "  'val_acc': 99.08151245117188},\n",
       " {'epoch': 51,\n",
       "  'time': 1.283951485005673,\n",
       "  'loss': 0.07831814349436562,\n",
       "  'acc': 97.2170601836906,\n",
       "  'val_loss': 0.030065083876252174,\n",
       "  'val_acc': 99.3111343383789},\n",
       " {'epoch': 52,\n",
       "  'time': 1.2723672469874145,\n",
       "  'loss': 0.07818785451414172,\n",
       "  'acc': 97.36647166102385,\n",
       "  'val_loss': 0.02946929633617401,\n",
       "  'val_acc': 99.3111343383789},\n",
       " {'epoch': 53,\n",
       "  'time': 1.4135102129948791,\n",
       "  'loss': 0.07957768895901925,\n",
       "  'acc': 96.94576421059853,\n",
       "  'val_loss': 0.034193024039268494,\n",
       "  'val_acc': 99.11978912353516},\n",
       " {'epoch': 54,\n",
       "  'time': 1.2777083859982667,\n",
       "  'loss': 0.07644062667838798,\n",
       "  'acc': 97.22748723305946,\n",
       "  'val_loss': 0.027828440070152283,\n",
       "  'val_acc': 99.29200744628906},\n",
       " {'epoch': 55,\n",
       "  'time': 1.2789141599932918,\n",
       "  'loss': 0.07946935607012638,\n",
       "  'acc': 96.95557907987232,\n",
       "  'val_loss': 0.06830774992704391,\n",
       "  'val_acc': 98.45005798339844},\n",
       " {'epoch': 56,\n",
       "  'time': 1.2817481930105714,\n",
       "  'loss': 0.08092325153060195,\n",
       "  'acc': 97.1950921933513,\n",
       "  'val_loss': 0.025059737265110016,\n",
       "  'val_acc': 99.42594909667969},\n",
       " {'epoch': 57,\n",
       "  'time': 1.454545482993126,\n",
       "  'loss': 0.07602458182445243,\n",
       "  'acc': 97.16978738327657,\n",
       "  'val_loss': 0.023270005360245705,\n",
       "  'val_acc': 99.46421813964844},\n",
       " {'epoch': 58,\n",
       "  'time': 1.313888902994222,\n",
       "  'loss': 0.08102856053054826,\n",
       "  'acc': 97.07962326175911,\n",
       "  'val_loss': 0.04436773061752319,\n",
       "  'val_acc': 98.9093017578125},\n",
       " {'epoch': 59,\n",
       "  'time': 1.2752850699907867,\n",
       "  'loss': 0.07822837440435551,\n",
       "  'acc': 97.1194304158865,\n",
       "  'val_loss': 0.02518860250711441,\n",
       "  'val_acc': 99.3876724243164},\n",
       " {'epoch': 60,\n",
       "  'time': 1.2961880959919654,\n",
       "  'loss': 0.0659076828848232,\n",
       "  'acc': 97.60735226465651,\n",
       "  'val_loss': 0.02573096752166748,\n",
       "  'val_acc': 99.36854553222656},\n",
       " {'epoch': 61,\n",
       "  'time': 1.403832138996222,\n",
       "  'loss': 0.06069835663334397,\n",
       "  'acc': 97.85267569802024,\n",
       "  'val_loss': 0.03189582750201225,\n",
       "  'val_acc': 99.04324340820312},\n",
       " {'epoch': 62,\n",
       "  'time': 1.2689216580038192,\n",
       "  'loss': 0.0624924586577849,\n",
       "  'acc': 97.7245339007417,\n",
       "  'val_loss': 0.028186814859509468,\n",
       "  'val_acc': 99.2345962524414},\n",
       " {'epoch': 63,\n",
       "  'time': 1.3010810150008183,\n",
       "  'loss': 0.065262437528692,\n",
       "  'acc': 97.70703465485376,\n",
       "  'val_loss': 0.0211603045463562,\n",
       "  'val_acc': 99.48335266113281},\n",
       " {'epoch': 64,\n",
       "  'time': 1.3989244929980487,\n",
       "  'loss': 0.05978013753644691,\n",
       "  'acc': 98.18069079690729,\n",
       "  'val_loss': 0.026366200298070908,\n",
       "  'val_acc': 99.25373077392578},\n",
       " {'epoch': 65,\n",
       "  'time': 1.3121066040039295,\n",
       "  'loss': 0.06137398445655492,\n",
       "  'acc': 97.68557688815535,\n",
       "  'val_loss': 0.03362852707505226,\n",
       "  'val_acc': 99.10064697265625},\n",
       " {'epoch': 66,\n",
       "  'time': 1.304397540006903,\n",
       "  'loss': 0.058587400750680405,\n",
       "  'acc': 98.15096806297618,\n",
       "  'val_loss': 0.01977432519197464,\n",
       "  'val_acc': 99.52162170410156},\n",
       " {'epoch': 67,\n",
       "  'time': 1.2941629169945372,\n",
       "  'loss': 0.0554201356635606,\n",
       "  'acc': 98.01869504313824,\n",
       "  'val_loss': 0.030178360641002655,\n",
       "  'val_acc': 99.10064697265625},\n",
       " {'epoch': 68,\n",
       "  'time': 1.3985367179993773,\n",
       "  'loss': 0.05892813672149969,\n",
       "  'acc': 98.0419704973205,\n",
       "  'val_loss': 0.023791197687387466,\n",
       "  'val_acc': 99.3111343383789},\n",
       " {'epoch': 69,\n",
       "  'time': 1.3250211050035432,\n",
       "  'loss': 0.05337883114876334,\n",
       "  'acc': 98.26563275550022,\n",
       "  'val_loss': 0.02150958776473999,\n",
       "  'val_acc': 99.34941101074219},\n",
       " {'epoch': 70,\n",
       "  'time': 1.2595470560045214,\n",
       "  'loss': 0.06392779847807135,\n",
       "  'acc': 97.61037684669179,\n",
       "  'val_loss': 0.02094011753797531,\n",
       "  'val_acc': 99.48335266113281},\n",
       " {'epoch': 71,\n",
       "  'time': 1.2641433469980257,\n",
       "  'loss': 0.05548559671098536,\n",
       "  'acc': 98.017497984831,\n",
       "  'val_loss': 0.02219271846115589,\n",
       "  'val_acc': 99.55989074707031},\n",
       " {'epoch': 72,\n",
       "  'time': 1.4209145229979185,\n",
       "  'loss': 0.0486304435170879,\n",
       "  'acc': 98.58857386565406,\n",
       "  'val_loss': 0.020154189318418503,\n",
       "  'val_acc': 99.48335266113281},\n",
       " {'epoch': 73,\n",
       "  'time': 1.278843947002315,\n",
       "  'loss': 0.0485801582555633,\n",
       "  'acc': 98.28999624961664,\n",
       "  'val_loss': 0.024493936449289322,\n",
       "  'val_acc': 99.42594909667969},\n",
       " {'epoch': 74,\n",
       "  'time': 1.270496484998148,\n",
       "  'loss': 0.04717883995680277,\n",
       "  'acc': 98.58614967283138,\n",
       "  'val_loss': 0.01798221841454506,\n",
       "  'val_acc': 99.52162170410156},\n",
       " {'epoch': 75,\n",
       "  'time': 1.2841456510068383,\n",
       "  'loss': 0.050884885975152006,\n",
       "  'acc': 98.26929568456224,\n",
       "  'val_loss': 0.019824478775262833,\n",
       "  'val_acc': 99.50248718261719},\n",
       " {'epoch': 76,\n",
       "  'time': 1.414005596001516,\n",
       "  'loss': 0.054173248920066294,\n",
       "  'acc': 97.98951437453593,\n",
       "  'val_loss': 0.03183268755674362,\n",
       "  'val_acc': 99.33026885986328},\n",
       " {'epoch': 77,\n",
       "  'time': 1.281943838999723,\n",
       "  'loss': 0.05052928651167341,\n",
       "  'acc': 98.45517787460453,\n",
       "  'val_loss': 0.01736624166369438,\n",
       "  'val_acc': 99.50248718261719},\n",
       " {'epoch': 78,\n",
       "  'time': 1.2722101849940373,\n",
       "  'loss': 0.056243777552173156,\n",
       "  'acc': 97.89305480452609,\n",
       "  'val_loss': 0.023064183071255684,\n",
       "  'val_acc': 99.48335266113281},\n",
       " {'epoch': 79,\n",
       "  'time': 1.2834800520067802,\n",
       "  'loss': 0.04572485144103854,\n",
       "  'acc': 98.41030171291887,\n",
       "  'val_loss': 0.01914447359740734,\n",
       "  'val_acc': 99.57903289794922},\n",
       " {'epoch': 80,\n",
       "  'time': 1.4397424570051953,\n",
       "  'loss': 0.042827372246783624,\n",
       "  'acc': 98.70436941099561,\n",
       "  'val_loss': 0.020922962576150894,\n",
       "  'val_acc': 99.42594909667969},\n",
       " {'epoch': 81,\n",
       "  'time': 1.271884392001084,\n",
       "  'loss': 0.042457682103657524,\n",
       "  'acc': 98.50032579406233,\n",
       "  'val_loss': 0.021487709134817123,\n",
       "  'val_acc': 99.52162170410156},\n",
       " {'epoch': 82,\n",
       "  'time': 1.2830325289978646,\n",
       "  'loss': 0.045835140322850756,\n",
       "  'acc': 98.49282944103904,\n",
       "  'val_loss': 0.018166812136769295,\n",
       "  'val_acc': 99.52162170410156},\n",
       " {'epoch': 83,\n",
       "  'time': 1.278366767000989,\n",
       "  'loss': 0.041808409754894985,\n",
       "  'acc': 98.5837749764939,\n",
       "  'val_loss': 0.02445262297987938,\n",
       "  'val_acc': 99.36854553222656},\n",
       " {'epoch': 84,\n",
       "  'time': 1.392734256995027,\n",
       "  'loss': 0.047535903443974896,\n",
       "  'acc': 98.28117521932302,\n",
       "  'val_loss': 0.02086271159350872,\n",
       "  'val_acc': 99.48335266113281},\n",
       " {'epoch': 85,\n",
       "  'time': 1.3477483799943002,\n",
       "  'loss': 0.059511433578719776,\n",
       "  'acc': 97.95875278189163,\n",
       "  'val_loss': 0.01827925816178322,\n",
       "  'val_acc': 99.59815979003906},\n",
       " {'epoch': 86,\n",
       "  'time': 1.2664973969949642,\n",
       "  'loss': 0.07442234413436621,\n",
       "  'acc': 97.28824735278926,\n",
       "  'val_loss': 0.025681884959340096,\n",
       "  'val_acc': 99.42594909667969},\n",
       " {'epoch': 87,\n",
       "  'time': 1.2795004570070887,\n",
       "  'loss': 0.052570209122640044,\n",
       "  'acc': 98.1222175408986,\n",
       "  'val_loss': 0.02893640659749508,\n",
       "  'val_acc': 99.40681457519531},\n",
       " {'epoch': 88,\n",
       "  'time': 1.423997663994669,\n",
       "  'loss': 0.05174583011914876,\n",
       "  'acc': 98.2438686780693,\n",
       "  'val_loss': 0.017639489844441414,\n",
       "  'val_acc': 99.61729431152344},\n",
       " {'epoch': 89,\n",
       "  'time': 1.3256501580035547,\n",
       "  'loss': 0.044512615593011715,\n",
       "  'acc': 98.47713249774019,\n",
       "  'val_loss': 0.02203334867954254,\n",
       "  'val_acc': 99.48335266113281},\n",
       " {'epoch': 90,\n",
       "  'time': 1.296736154006794,\n",
       "  'loss': 0.04271144295219055,\n",
       "  'acc': 98.60080655941293,\n",
       "  'val_loss': 0.01747681200504303,\n",
       "  'val_acc': 99.57903289794922},\n",
       " {'epoch': 91,\n",
       "  'time': 1.2663579460058827,\n",
       "  'loss': 0.03724987918803514,\n",
       "  'acc': 98.9174002024753,\n",
       "  'val_loss': 0.01921691745519638,\n",
       "  'val_acc': 99.54075622558594},\n",
       " {'epoch': 92,\n",
       "  'time': 1.3970821969996905,\n",
       "  'loss': 0.03537514353217172,\n",
       "  'acc': 98.9486730906589,\n",
       "  'val_loss': 0.017026569694280624,\n",
       "  'val_acc': 99.59815979003906},\n",
       " {'epoch': 93,\n",
       "  'time': 1.2761092330038082,\n",
       "  'loss': 0.03436752331774097,\n",
       "  'acc': 99.01376342773438,\n",
       "  'val_loss': 0.018712328746914864,\n",
       "  'val_acc': 99.55989074707031},\n",
       " {'epoch': 94,\n",
       "  'time': 1.2685487880080473,\n",
       "  'loss': 0.03588955556927634,\n",
       "  'acc': 98.949702491445,\n",
       "  'val_loss': 0.016390928998589516,\n",
       "  'val_acc': 99.57903289794922},\n",
       " {'epoch': 95,\n",
       "  'time': 1.2622388180025155,\n",
       "  'loss': 0.03725624170677721,\n",
       "  'acc': 98.90680328873563,\n",
       "  'val_loss': 0.02045000158250332,\n",
       "  'val_acc': 99.40681457519531},\n",
       " {'epoch': 96,\n",
       "  'time': 1.41073022299679,\n",
       "  'loss': 0.03500061710018757,\n",
       "  'acc': 98.93385113172295,\n",
       "  'val_loss': 0.018657388165593147,\n",
       "  'val_acc': 99.57903289794922},\n",
       " {'epoch': 97,\n",
       "  'time': 1.2899901259952458,\n",
       "  'loss': 0.03547545248442445,\n",
       "  'acc': 98.90592395372627,\n",
       "  'val_loss': 0.016703352332115173,\n",
       "  'val_acc': 99.59815979003906},\n",
       " {'epoch': 98,\n",
       "  'time': 1.281385712994961,\n",
       "  'loss': 0.03726556022797734,\n",
       "  'acc': 98.86079217579739,\n",
       "  'val_loss': 0.018329620361328125,\n",
       "  'val_acc': 99.59815979003906},\n",
       " {'epoch': 99,\n",
       "  'time': 1.2736716310027987,\n",
       "  'loss': 0.03592491670211485,\n",
       "  'acc': 98.8046956338173,\n",
       "  'val_loss': 0.01841062493622303,\n",
       "  'val_acc': 99.65557098388672},\n",
       " {'epoch': 100,\n",
       "  'time': 1.4177122980036074,\n",
       "  'loss': 0.03269902949244523,\n",
       "  'acc': 99.11702810240186,\n",
       "  'val_loss': 0.014298935420811176,\n",
       "  'val_acc': 99.7512435913086}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.AdamW(full_network.parameters(), lr)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Model(\n",
    "    full_network,\n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    batch_metrics=['accuracy'],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model.fit_generator(train_loader, valid_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = PageDataset([\"../data/train/xx-2020-04-20-RGL-1289-Formation-CCE-adopte_1.csv\"], predict=True)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"../data/train/xx-2020-04-20-RGL-1289-Formation-CCE-adopte_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([382, 11])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = [page for _, page in test_df.groupby(\"page\")]\n",
    "pages[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn_predict(batch):\n",
    "    # Discard bogus (or unwanted) tags\n",
    "    sequences_vectors, lengths = zip(\n",
    "        *[\n",
    "            (torch.FloatTensor(seq_vectors), len(seq_vectors))\n",
    "            for seq_vectors, _ in sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    padded_sequences_vectors = pad_sequence(sequences_vectors, batch_first=True, padding_value=0)\n",
    "    pack_padded_sequences_vectors = pack_padded_sequence(padded_sequences_vectors, lengths.cpu(), batch_first=True)\n",
    "\n",
    "    return pack_padded_sequences_vectors\n",
    "\n",
    "predict_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=pad_collate_fn_predict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction steps: 1 0.03s                          \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_dataset(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=pad_collate_fn_predict,\n",
    "    concatenate_returns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 13, 383)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_predictions = []\n",
    "for batch in predictions:\n",
    "    idx_predictions.extend(batch.argmax(axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = dict((v, k) for k, v in ds.vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Section',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Enumeration',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'B-Alinea',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'O',\n",
       " 'I',\n",
       " 'I',\n",
       " 'O',\n",
       " 'I',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_predictions = []\n",
    "for page in idx_predictions:\n",
    "    tags_predictions.append([inv_vocab.get(tag) for tag in page])\n",
    "tags_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['I', 'RGLEMENT'],\n",
       "       ['I', '1289'],\n",
       "       ['I', 'concernant'],\n",
       "       ['I', 'formation'],\n",
       "       ['I', 'dun'],\n",
       "       ['I', 'Comit'],\n",
       "       ['I', 'consultatif'],\n",
       "       ['I', 'en'],\n",
       "       ['I', 'environnement'],\n",
       "       ['B-Article', 'Sance'],\n",
       "       ['O', 'ordinaire'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'conseil'],\n",
       "       ['O', 'municipal,'],\n",
       "       ['O', 'tenue'],\n",
       "       ['O', ''],\n",
       "       ['O', 'huis'],\n",
       "       ['O', 'clos'],\n",
       "       ['O', 'le'],\n",
       "       ['O', '20'],\n",
       "       ['O', 'avril'],\n",
       "       ['O', '2020'],\n",
       "       ['O', ''],\n",
       "       ['O', '19'],\n",
       "       ['O', 'h,'],\n",
       "       ['O', 'dans'],\n",
       "       ['O', 'la'],\n",
       "       ['O', 'salle'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'conseil'],\n",
       "       ['O', 'municipal'],\n",
       "       ['O', 'situe'],\n",
       "       ['I', 'au'],\n",
       "       ['I', '1386,'],\n",
       "       ['I', 'rue'],\n",
       "       ['I', 'Dumouchel,'],\n",
       "       ['O', 'Sainte-Adle,'],\n",
       "       ['O', 'lieu'],\n",
       "       ['I', 'ordinaire'],\n",
       "       ['O', 'des'],\n",
       "       ['O', 'sances'],\n",
       "       ['O', ''],\n",
       "       ['O', 'laquelle'],\n",
       "       ['O', 'sont'],\n",
       "       ['O', 'prsents'],\n",
       "       ['O', 'les'],\n",
       "       ['O', 'membres'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'conseil'],\n",
       "       ['O', 'suivants'],\n",
       "       ['O', 'formant'],\n",
       "       ['O', 'le'],\n",
       "       ['O', 'quorum'],\n",
       "       ['O', ':'],\n",
       "       ['O', 'Monsieur'],\n",
       "       ['O', 'Pierre'],\n",
       "       ['O', 'Lafond'],\n",
       "       ['O', 'Conseiller'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'district'],\n",
       "       ['O', '1'],\n",
       "       ['O', 'Monsieur'],\n",
       "       ['O', 'Martin'],\n",
       "       ['O', 'Jolicoeur'],\n",
       "       ['O', 'Conseiller'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'district'],\n",
       "       ['O', '4'],\n",
       "       ['O', 'Madame'],\n",
       "       ['O', 'Frdrike'],\n",
       "       ['O', 'Cavezzali'],\n",
       "       ['O', 'Conseillre'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'district'],\n",
       "       ['O', '5'],\n",
       "       ['O', 'Madame'],\n",
       "       ['O', 'Cline'],\n",
       "       ['O', 'Dor'],\n",
       "       ['O', 'Conseillre'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'district'],\n",
       "       ['O', '6'],\n",
       "       ['O', 'sous'],\n",
       "       ['O', 'la'],\n",
       "       ['O', 'prsidence'],\n",
       "       ['O', 'de'],\n",
       "       ['O', 'madame'],\n",
       "       ['O', 'la'],\n",
       "       ['O', 'mairesse'],\n",
       "       ['O', 'Nadine'],\n",
       "       ['O', 'Brire.'],\n",
       "       ['O', 'Messieurs'],\n",
       "       ['I', 'les'],\n",
       "       ['O', 'conseillers'],\n",
       "       ['O', 'Roch'],\n",
       "       ['O', 'Bdard'],\n",
       "       ['O', 'et'],\n",
       "       ['O', 'Robert'],\n",
       "       ['O', 'Blisle'],\n",
       "       ['O', 'sont'],\n",
       "       ['O', 'absents'],\n",
       "       ['O', 'pour'],\n",
       "       ['O', 'toute'],\n",
       "       ['O', 'la'],\n",
       "       ['O', 'dure'],\n",
       "       ['O', 'de'],\n",
       "       ['O', 'la'],\n",
       "       ['O', 'sance.'],\n",
       "       ['O', 'ATTENDU'],\n",
       "       ['I', \"QU'un\"],\n",
       "       ['I', 'avis'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'motion'],\n",
       "       ['I', 'du'],\n",
       "       ['I', 'prsent'],\n",
       "       ['I', 'rglement'],\n",
       "       ['O', 'a'],\n",
       "       ['O', 't'],\n",
       "       ['O', 'donn'],\n",
       "       ['O', 'lors'],\n",
       "       ['O', 'de'],\n",
       "       ['O', 'la'],\n",
       "       ['I', 'sance'],\n",
       "       ['O', 'ordinaire'],\n",
       "       ['I', 'du'],\n",
       "       ['O', '16'],\n",
       "       ['O', 'mars'],\n",
       "       ['O', '2020'],\n",
       "       ['O', 'par'],\n",
       "       ['O', 'madame'],\n",
       "       ['O', 'la'],\n",
       "       ['O', 'conseillre'],\n",
       "       ['O', 'Cline'],\n",
       "       ['O', 'Dor'],\n",
       "       ['O', ';'],\n",
       "       ['O', 'ATTENDU'],\n",
       "       ['O', 'QUE'],\n",
       "       ['O', 'le'],\n",
       "       ['O', 'projet'],\n",
       "       ['O', 'de'],\n",
       "       ['O', 'rglement'],\n",
       "       ['O', 'a'],\n",
       "       ['O', 't'],\n",
       "       ['O', 'dpos'],\n",
       "       ['O', 'et'],\n",
       "       ['O', 'prsent'],\n",
       "       ['O', 'avec'],\n",
       "       ['O', 'lavis'],\n",
       "       ['O', 'de'],\n",
       "       ['O', 'motion'],\n",
       "       ['O', ';'],\n",
       "       ['O', 'ATTENDU'],\n",
       "       ['O', 'QUune'],\n",
       "       ['O', 'copie'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'rglement'],\n",
       "       ['O', 'a'],\n",
       "       ['O', 't'],\n",
       "       ['O', 'remise'],\n",
       "       ['O', 'aux'],\n",
       "       ['O', 'membres'],\n",
       "       ['O', 'du'],\n",
       "       ['O', 'conseil'],\n",
       "       ['O', 'municipal'],\n",
       "       ['I', '3'],\n",
       "       ['O', 'jours'],\n",
       "       ['I', 'ouvrables'],\n",
       "       ['I', 'avant'],\n",
       "       ['I', 'la'],\n",
       "       ['I', 'prsente'],\n",
       "       ['O', 'sance'],\n",
       "       ['O', ';'],\n",
       "       ['O', 'LE'],\n",
       "       ['I', 'CONSEIL'],\n",
       "       ['I', 'MUNICIPAL'],\n",
       "       ['I', 'dcrte'],\n",
       "       ['I', 'quil'],\n",
       "       ['I', 'soit'],\n",
       "       ['I', 'statu'],\n",
       "       ['I', 'et'],\n",
       "       ['I', 'ordonn'],\n",
       "       ['I', 'par'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'prsent'],\n",
       "       ['I', 'rglement'],\n",
       "       ['I', 'ce'],\n",
       "       ['I', 'qui'],\n",
       "       ['I', 'suit'],\n",
       "       ['O', ':'],\n",
       "       ['B-Alinea', 'Article'],\n",
       "       ['I', '1'],\n",
       "       ['I', 'Formation'],\n",
       "       ['I', 'et'],\n",
       "       ['I', 'nom'],\n",
       "       ['B-Alinea', 'Un'],\n",
       "       ['I', 'comit'],\n",
       "       ['I', 'est'],\n",
       "       ['I', 'constitu,'],\n",
       "       ['I', 'par'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'prsent'],\n",
       "       ['I', 'rglement,'],\n",
       "       ['I', 'sous'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'nom'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'Comit'],\n",
       "       ['I', 'consultatif'],\n",
       "       ['I', 'en'],\n",
       "       ['I', 'environnement.'],\n",
       "       ['B-Alinea', 'Article'],\n",
       "       ['I', '2'],\n",
       "       ['I', 'Composition'],\n",
       "       ['B-Alinea', 'Par'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'prsent'],\n",
       "       ['I', 'rglement,'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'conseil'],\n",
       "       ['I', 'est'],\n",
       "       ['I', 'autoris'],\n",
       "       ['I', ''],\n",
       "       ['I', 'nommer,'],\n",
       "       ['I', 'par'],\n",
       "       ['I', 'rsolution,'],\n",
       "       ['I', 'tous'],\n",
       "       ['I', 'les'],\n",
       "       ['I', 'membres'],\n",
       "       ['I', 'du'],\n",
       "       ['I', 'comit'],\n",
       "       ['I', 'selon'],\n",
       "       ['I', 'les'],\n",
       "       ['I', 'dispositions'],\n",
       "       ['I', 'ci-aprs'],\n",
       "       ['I', ':'],\n",
       "       ['B-Alinea', '\\uf0a7'],\n",
       "       ['I', 'Un'],\n",
       "       ['I', 'membre'],\n",
       "       ['I', 'du'],\n",
       "       ['I', 'conseil'],\n",
       "       ['I', 'municipal'],\n",
       "       ['I', ';'],\n",
       "       ['B-Enumeration', '\\uf0a7'],\n",
       "       ['I', 'Un'],\n",
       "       ['I', 'nombre'],\n",
       "       ['I', 'minimal'],\n",
       "       ['I', 'de'],\n",
       "       ['I', '3'],\n",
       "       ['I', 'membres'],\n",
       "       ['I', 'citoyens'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'la'],\n",
       "       ['I', 'Ville'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'Sainte-Adle'],\n",
       "       ['I', 'ou'],\n",
       "       ['I', 'reprsentant'],\n",
       "       ['I', 'dune'],\n",
       "       ['I', 'entreprise'],\n",
       "       ['I', 'ayant'],\n",
       "       ['I', 'sa'],\n",
       "       ['I', 'place'],\n",
       "       ['I', 'daffaires'],\n",
       "       ['I', 'sur'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'territoire'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'la'],\n",
       "       ['I', 'ville'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'Sainte-Adle'],\n",
       "       ['I', ';'],\n",
       "       ['B-Enumeration', '\\uf0a7'],\n",
       "       ['I', 'Un'],\n",
       "       ['I', 'fonctionnaire'],\n",
       "       ['I', 'municipal,'],\n",
       "       ['I', 'lequel'],\n",
       "       ['I', 'na'],\n",
       "       ['I', 'pas'],\n",
       "       ['I', ''],\n",
       "       ['I', 'tre'],\n",
       "       ['I', 'nomm'],\n",
       "       ['I', 'par'],\n",
       "       ['I', 'rsolution,'],\n",
       "       ['I', 'agissant'],\n",
       "       ['I', ''],\n",
       "       ['I', 'la'],\n",
       "       ['I', 'fois'],\n",
       "       ['I', 'comme'],\n",
       "       ['I', 'membre,'],\n",
       "       ['I', 'conseiller'],\n",
       "       ['I', 'et'],\n",
       "       ['I', 'secrtaire'],\n",
       "       ['I', ';'],\n",
       "       ['B-Alinea', 'Article'],\n",
       "       ['I', '3'],\n",
       "       ['I', 'Dure'],\n",
       "       ['I', 'du'],\n",
       "       ['I', 'mandat'],\n",
       "       ['B-Alinea', 'Le'],\n",
       "       ['I', 'mandat'],\n",
       "       ['I', 'des'],\n",
       "       ['I', 'membres'],\n",
       "       ['I', 'est'],\n",
       "       ['I', 'valide'],\n",
       "       ['I', 'tant'],\n",
       "       ['I', 'et'],\n",
       "       ['I', 'aussi'],\n",
       "       ['I', 'longtemps'],\n",
       "       ['I', 'que'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'conseil'],\n",
       "       ['I', 'nen'],\n",
       "       ['I', 'dcide'],\n",
       "       ['I', 'autrement.'],\n",
       "       ['B-Alinea', 'Article'],\n",
       "       ['I', '4'],\n",
       "       ['I', 'Mandat'],\n",
       "       ['I', 'du'],\n",
       "       ['I', 'comit'],\n",
       "       ['B-Alinea', 'Le'],\n",
       "       ['I', 'comit'],\n",
       "       ['I', 'doit'],\n",
       "       ['I', ':'],\n",
       "       ['B-Enumeration', 'a)'],\n",
       "       ['I', 'tudier'],\n",
       "       ['I', 'toute'],\n",
       "       ['I', 'question'],\n",
       "       ['I', 'en'],\n",
       "       ['I', 'matire'],\n",
       "       ['I', 'environnementale'],\n",
       "       ['I', 'que'],\n",
       "       ['I', 'lui'],\n",
       "       ['I', 'soumet'],\n",
       "       ['I', 'le'],\n",
       "       ['I', 'conseil'],\n",
       "       ['I', 'municipal'],\n",
       "       ['I', 'et'],\n",
       "       ['I', 'faire'],\n",
       "       ['I', 'rapport'],\n",
       "       ['I', ''],\n",
       "       ['I', 'ce'],\n",
       "       ['I', 'dernier'],\n",
       "       ['I', 'dans'],\n",
       "       ['I', 'les'],\n",
       "       ['I', 'dlais'],\n",
       "       ['I', 'fixs'],\n",
       "       ['I', 'par'],\n",
       "       ['I', 'celui-'],\n",
       "       ['I', 'ci'],\n",
       "       ['I', ';'],\n",
       "       ['B-Enumeration', 'b)'],\n",
       "       ['I', 'participer'],\n",
       "       ['I', 'aux'],\n",
       "       ['I', 'exercices'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'consultation'],\n",
       "       ['I', 'publique'],\n",
       "       ['I', 'de'],\n",
       "       ['I', 'la'],\n",
       "       ['I', 'Ville'],\n",
       "       ['I', 'en'],\n",
       "       ['I', 'matire'],\n",
       "       ['I', 'denvironnement'],\n",
       "       ['I', ';'],\n",
       "       ['B-Enumeration', 'c)'],\n",
       "       ['I', 'tudier'],\n",
       "       ['I', 'et'],\n",
       "       ['I', 'effectuer'],\n",
       "       ['I', 'des'],\n",
       "       ['I', 'recommandations'],\n",
       "       ['I', 'sur'],\n",
       "       ['I', 'les'],\n",
       "       ['I', 'projets'],\n",
       "       ['I', 'prsents'],\n",
       "       ['I', 'en'],\n",
       "       ['I', 'vertu'],\n",
       "       ['I', 'du'],\n",
       "       ['I', 'fond'],\n",
       "       ['I', 'municipal'],\n",
       "       ['I', 'vert'],\n",
       "       ['I', ';']], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].assign(tag=tags_predictions[1][:len(pages[0])]).loc[:, [\"tag\", \"text\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
